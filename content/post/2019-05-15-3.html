---
title: 高斯混合模型 3
author: Yi LIU
date: '2019-05-15'
slug: '3'
categories:
  - Work
tags:
  - Bayesian
  - Unsupervised learning
output:
  blogdown::html_page:
    toc: true
    toc_depth: 2
bibliography: [Refs.bib]
biblio-style: "apalike"
link-citations: true
---


<div id="TOC">
<ul>
<li><a>简介</a></li>
<li><a href="#em">EM算法的一般定义</a></li>
<li><a href="#em">为什么EM算法有效？</a></li>
<li><a href="#em">EM算法的一些扩展</a></li>
<li><a>小结</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<div class="section level1">
<h1>简介</h1>
<p>在前面几篇文章里面，我们介绍了高斯混合模型的定义以及优化求解算法。我们提到EM算法是一种常用的求解高斯混合模型参数的高效算法。在本篇文章里，我们会对EM算法做更进一步的介绍。</p>
</div>
<div id="em" class="section level1">
<h1>EM算法的一般定义</h1>
<p>在高斯混合模型中使用EM算法时，我们引入了隐变量<span class="math inline">\(z\)</span>，首先计算了它在目前参数值下的期望值（E-step），然后利用这个期望值计算全数据似然函数并更新参数（M-step）。实际上，这个算法做的是下面一件事情：在每一次迭代<span class="math inline">\(t\)</span>中，我们定义了一个原来似然函数的代替函数（surrogate function） <span class="math display">\[Q(\theta|\theta^{(t)}) = \sum_z Pr(z|x,\theta^{(t)})\log Pr(x, z|\theta)\]</span> 然后在每步迭代中都优化<span class="math inline">\(\theta\)</span>。把上面的式子展开就得到 <span class="math display">\[\begin{split}
&amp; Q(\theta|\theta^{(t)}) \\
= &amp; \sum_z \sum_i \sum_k Pr(z|x,\theta^{(t)}) z_{ik} \left(\log \pi_k + \log\left[\phi(x_i|\mu_k, \sigma_k^2) \right]\right) \\
= &amp; \sum_i \sum_k \left(\sum_z Pr(z|x,\theta^{(t)}) z_{ik}\right) \left(\log \pi_k + \log\left[\phi(x_i|\mu_k, \sigma_k^2) \right]\right) \\
= &amp; \sum_i \sum_k E(z|x,\theta^{(t)})\left(\log \pi_k + \log\left[\phi(x_i|\mu_k, \sigma_k^2) \right]\right) \\
= &amp; \sum_i \sum_k \gamma_{ik}\left(\log \pi_k + \log\left[\phi(x_i|\mu_k, \sigma_k^2) \right]\right)
\end{split}\]</span> 最后一行，正是我们前一篇文章里介绍的算法里优化的函数。</p>
<p>一般意义下的EM算法是这样的：对于一个问题，我们有观察值<span class="math inline">\(x\)</span>，想要估计参数<span class="math inline">\(\theta\)</span>使得对数似然函数<span class="math inline">\(\ell(\theta|x) = \log Pr(x|\theta)\)</span>最大。EM算法的做法是先引入隐变量<span class="math inline">\(z\)</span>,然后迭代以下两步</p>
<ol style="list-style-type: decimal">
<li>E-step：计算隐变量的后验概率<span class="math inline">\(Pr(z|x, \theta^{(t)})\)</span></li>
<li>M-step：定义函数<span class="math inline">\(Q(\theta|\theta^{(t)}) = \sum_z Pr(z|x, \theta^{(t)}) \log Pr(x,z|\theta)\)</span>，更新参数 <span class="math display">\[\theta^{(t+1)} = \arg \max_\theta Q(\theta| \theta^{(t)}) \]</span></li>
</ol>
<p>EM算法在M-step时通常比原来的问题要简单，是因为对数函数作用在了全数据似然函数上面，得到的结果比较简单。</p>
</div>
<div id="em" class="section level1">
<h1>为什么EM算法有效？</h1>
<p>要论证EM算法是有效的，我们要证明在每一次迭代当中EM算法确实增加了对数似然函数。首先我们把对数似然函数利用隐变量改写一下 <span class="math display">\[\ell(\theta|x) = \log Pr(x|\theta) = \log Pr(x,z|\theta) - \log Pr(z|x, \theta)\]</span> 因为对数似然函数和隐变量<span class="math inline">\(z\)</span>无关，我们可以把上式两边都乘以一个<span class="math inline">\(z\)</span>的概率分布<span class="math inline">\(q(z)\)</span>，并对<span class="math inline">\(z\)</span>求期望。这时左边还是原来的值，从而得到 <span class="math display">\[\begin{split} &amp; \ell(\theta|x) \\
= &amp; \sum_z q(z) \log Pr(x,z|\theta) - \sum_z q(z) \log Pr(z|x, \theta) \\
= &amp; \sum_z q(z) \log \frac{Pr(x,z|\theta)}{q(z)} - \sum_z q(z) \log \frac{Pr(z|x, \theta)}{q(z)} \\
= &amp; \mathcal{L}(q, \theta) + KL(q(z)||Pr(z|x, \theta))
\end{split}\]</span> 因为KL散度始终非负，我们有 <span class="math display">\[\ell(\theta|x) \geq \mathcal{L}(q, \theta)\]</span> 因此，<span class="math inline">\(\mathcal{L}(q, \theta)\)</span>是<span class="math inline">\(\ell(\theta|x)\)</span>的下界（lower bound）。并且，如果我们取<span class="math inline">\(q(z) = Pr(z|x, \theta^{(t)})\)</span>，那么就有<span class="math inline">\(\mathcal{L}(Pr(z|x, \theta^{(t)}), \theta) = Q(\theta|\theta^{(t)}) + \sum_z Pr(z|x, \theta^{(t)}) \log Pr(z|x, \theta^{(t)})\)</span></p>
<p>对于第<span class="math inline">\(t\)</span>次迭代，在E-step中，我们选取<span class="math inline">\(q^{(t)}(z)=Pr(z|x, \theta^{(t)})\)</span>，这就使上式中的第二项KL散度为0。因此，我们有 <span class="math display">\[\begin{split}
&amp; \ell(\theta^{(t)}|x)\\
= &amp; \mathcal{L}(Pr(z|x, \theta^{(t)}), \theta) + \underbrace{KL(Pr(z|x, \theta^{(t)})||Pr(z|x, \theta^{(t)}))}_{=0} \\
= &amp; \mathcal{L}(Pr(z|x, \theta^{(t)}), \theta) \\
= &amp; Q(\theta|\theta^{(t)}) + \underbrace{\sum_z Pr(z|x, \theta^{(t)}) \log Pr(z|x, \theta^{(t)})}_{\text{indeptendent of }\theta}
\end{split}\]</span></p>
<p>在M-step中，我们对于这个下界优化求解（也就是对<span class="math inline">\(Q\)</span>优化，因为上式第二项与<span class="math inline">\(\theta\)</span>无关），并更新参数 <span class="math display">\[\theta^{(t+1)} = \arg\max_\theta \mathcal{L}(Pr(z|x, \theta^{(t)}), \theta) =\arg\max_\theta Q(\theta|\theta^{(t)})\]</span> 因此我们有 <span class="math display">\[\begin{split}
&amp; \ell(\theta^{(t+1)}|x)\\
\geq &amp; \mathcal{L}(Pr(z|x, \theta^{(t)}), \theta^{(t+1)}) \quad (\text{KL散度非负})\\
\geq &amp; \mathcal{L}(Pr(z|x, \theta^{(t)}), \theta^{(t)}) \quad (\text{优化求解结果})\\
=&amp; \ell(\theta^{(t)}|x) \quad (\text{E-step结果})
\end{split}\]</span></p>
<p>所以，我们就证明了EM算法确实有效。</p>
</div>
<div id="em" class="section level1">
<h1>EM算法的一些扩展</h1>
<p>EM算法解决的问题是当<span class="math inline">\(\ell(\theta|x)\)</span>很难优化时，通过引入隐变量<span class="math inline">\(z\)</span>，把对数似然函数分解为一个KL散度和一个原对数似然函数下界<span class="math inline">\(\mathcal{L}(q, \theta)\)</span>之和。在迭代优化的E-step中，取<span class="math inline">\(q(z)=Pr(z|x,\theta^{(t)})\)</span>使KL散度为<span class="math inline">\(0\)</span>，最大化了<span class="math inline">\(\mathcal{L}(q, \theta)\)</span>。在M-step中，再最大化这个下界，更新参数 <span class="math display">\[\theta^{(t+1)} = \arg\max_\theta \mathcal{L}(Pr(z|x,\theta^{(t)}), \theta)\]</span></p>
<p>其实我们是为了优化<span class="math inline">\(\ell(\theta|x)\)</span>定义了一个函数<span class="math inline">\(g(\theta|\theta^{(t)}) =\mathcal{L}(Pr(z|x,\theta^{(t)}), \theta)\)</span>，它满足下面两个条件：</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(g(\theta|\theta^{(t)}) \leq \ell(\theta|x)\)</span></li>
<li><span class="math inline">\(g(\theta^{(t)}|\theta^{(t)}) = \ell(\theta^{(t)}|x)\)</span></li>
</ol>
<p>这样我们说<span class="math inline">\(g(\theta|\theta^{(t)})\)</span>是<span class="math inline">\(\ell(\theta|x)\)</span>在<span class="math inline">\(\theta^{(t)}\)</span>处的下界（<span class="math inline">\(g(\theta|\theta^{(t)})\)</span> minorize <span class="math inline">\(\ell(\theta|x)\)</span> at <span class="math inline">\(\theta^{(t)}\)</span>）。</p>
<p>如果能够选取<span class="math inline">\(\theta^{(t+1)}\)</span>使得<span class="math inline">\(\theta^{(t+1)}=\arg\max_\theta g(\theta|\theta^{(t)})\)</span>，那么我们就得到 <span class="math display">\[\begin{split}
&amp; \ell(\theta^{(t+1)}|x) \\
\geq &amp; g(\theta^{(t+1)}|\theta^{(t)}) \\
\geq &amp; g(\theta^{(t)}|\theta^{(t)}) \\
= &amp; \ell(\theta^{(t)}|x)
\end{split}\]</span> 这样我们就可以优化求解<span class="math inline">\(\ell(\theta^{(t+1)}|x)\)</span>。上面的EM算法是一个特例，MM算法还有很多其它的形式，具体可以参看 <span class="citation">Hunter and Lange (<a href="#ref-MM2004">2004</a>)</span></p>
</div>
<div class="section level1">
<h1>小结</h1>
<p>在本篇文章里，我们从前一篇文章里的高斯混合模型的EM算法出发，引入了EM算法的一般定义，并且证明了它的有效性。这里的内容很大程度上参考了 <span class="citation">Bishop (<a href="#ref-Bishop2006">2006</a>)</span>。</p>
<p>此外，我们还引入了EM算法的推广，MM算法。对于这个算法本身我们并没有过多介绍，如果有需要可以参考文献 <span class="citation">Hunter and Lange (<a href="#ref-MM2004">2004</a>)</span>。</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-Bishop2006">
<p>Bishop, Christopher M. 2006. <em>Pattern Recognition and Machine Learning (Information Science and Statistics)</em>. Berlin, Heidelberg: Springer-Verlag.</p>
</div>
<div id="ref-MM2004">
<p>Hunter, David R, and Kenneth Lange. 2004. “A Tutorial on Mm Algorithms.” <em>The American Statistician</em> 58 (1). Taylor &amp; Francis: 30–37.</p>
</div>
</div>
</div>
