---
title: 高斯混合模型 2
author: Yi LIU
date: '2019-04-27'
slug: '2'
categories:
  - Work
tags:
  - Bayesian
  - Stats
  - Unsupervised learning
output:
  blogdown::html_page:
    toc: true
---


<div id="TOC">
<ul>
<li><a>简介</a></li>
<li><a>算法细节</a></li>
<li><a>算法实现</a></li>
<li><a>为什么上面的算法是正确的</a></li>
<li><a>小结</a></li>
</ul>
</div>

<div class="section level1">
<h1>简介</h1>
<p>在上一篇文章中，我们简单给出了高斯模型的定义，以及说明梯度上升方法并不是求解这个问题的参数的高效方法。在本篇文章里，我们给出一个高效的求解算法，EM算法，并给出一些背后的理论推导。</p>
</div>
<div class="section level1">
<h1>算法细节</h1>
<p>对于高斯混合模型来说，我们要估计的参数有：</p>
<ul>
<li><span class="math inline">\(\pi_k, k \in \{1, \dots, K\}\)</span>： 各高斯组分比例<span class="math inline">\(\pi_k &gt; 0, \sum_k \pi_k=1\)</span></li>
<li><span class="math inline">\(\mu_k, k \in \{1, \dots, K\}\)</span>：各高斯组分均值</li>
<li><span class="math inline">\(\sigma_k^2, k \in \{1, \dots, K\}\)</span>：各高斯组分方差</li>
</ul>
<p>我们求解方法是最大似然估计（maximum (log-)likelihood estimation, MLE）。对数似然函数是 <span class="math display">\[ \ell(\theta|x) = \sum_{i=1}^n \log \left( \sum_k \pi_k \phi_k(x_i) \right)\]</span></p>
<p>在前一篇文章里面我们已经简略提过，对于高斯混合模型求解，比较好的方法是Expectation Maximization（EM）算法。对于每个观察值<span class="math inline">\(x_i\)</span>来说，我们并不知道它属于那个组分，为此，我们引入一个因变量，组分指示器(indicator)<span class="math inline">\(z_{ik}\)</span>。它的取值如下 <span class="math display">\[z_{ik} = \left\{\begin{split} &amp;1,&amp; \text{ if }x_i \in k\text{ th-comp} \\ &amp;0, &amp; \text{ otherwise} \end{split}\right.\]</span> 当我们能够观测到它的值的时候，我们有全数据似然函数 <span class="math display">\[L_{complete}(\theta| x, z)=\prod_i \prod_k (\pi_k \phi(x_i|\mu_k, \sigma_k^2))^{z_{ik}}\]</span> 相应的对数似然函数是 <span class="math display">\[\ell_{complete}(\theta| x, z)=\sum_i \sum_k z_{ik}\left(\log \pi_k + \log\left[\phi(x_i|\mu_k, \sigma_k^2) \right]\right)\]</span> 这时候参数估计非常简单。对于组分的均值和方差来说，就是简单的单高斯分布参数估计。对于组分比例来说，就是<span class="math inline">\(\hat{\pi_k} = \sum_i z_{ik}/N\)</span>。</p>
<p>但是实际情况是我们不知道<span class="math inline">\(z_{ik}\)</span>的值，因此我们必须使用EM算法。EM算法大致步骤如下：</p>
<ol style="list-style-type: decimal">
<li>初始化参数<span class="math inline">\(\pi_k = 1/K\)</span>, <span class="math inline">\(\sigma_k^2 = \sum_i(x_i - \bar{x}_i)^2\)</span></li>
<li>基于当前参数估计<span class="math inline">\(\theta^{(t)}\)</span>迭代以下两步直至收敛：
<ol style="list-style-type: lower-alpha">
<li>E-step：计算在当前参数下隐变量<span class="math inline">\(z_{ik}\)</span>的期望(expectation)，<span class="math inline">\(E(z_{ik}|x, \theta^{(t)})=Pr(x_i \in k\text{-th comp}|\theta^{(t)}) := \gamma_{ik}\)</span></li>
<li>M-step：最大化对数似然函数在给定隐变量期望下的参数，并更新。<span class="math inline">\(\theta^{(t+1)} = \arg\max_\theta \ell_{complete}(\theta|x, z=E(z|x, \theta^{(t)}))\)</span></li>
</ol></li>
</ol>
<p>对于高斯混合模型来说，下面给出参数更新具体计算步骤。计算隐变量期望的表达式如下： <span class="math display">\[\begin{split}&amp; \gamma_{ik} \\
= &amp; E(z_{ik}|x, \theta^{(t)})\\
= &amp; Pr(x_i \in k\text{-th comp}|\theta^{(t)}) \\ 
= &amp; Pr(z_{ik}=1|x_i, \theta^{(t)}) \\
= &amp; \frac{Pr(z_{ik}=1, x_i| \theta^{(t)})}{ Pr(x_i|\theta^{(t)})} \\
= &amp; \frac{Pr(z_{ik}=1|\theta^{(t)}) Pr(x_i|z_{ik=1}, \theta^{(t)})}{Pr(x_i|\theta^{(t)})} \\
= &amp; \frac{\pi_k^{(t)} \phi(x_i|\mu_k^{(t)}, (\sigma_k^2)^{(t)})}{\sum_l \pi_l^{(t)} \phi(x_i|\mu_l^{(t)}, (\sigma_l^2)^{(t)})}
\end{split}\]</span></p>
<p>对于更新参数，我们有 <span class="math display">\[\ell_{complete}(\theta|x, E(z|\theta^{(t)})) = \sum_i \sum_k \gamma_{ik} \left(\log \pi_k + \log\left[\phi(x_i|\mu_k, \sigma_k^2) \right]\right)\]</span> 对其中的参数求偏导并令其为零，并考虑约束条件<span class="math inline">\(\sum_k \pi_k=1\)</span>可解得 <span class="math display">\[\pi_k^{(t+1)} = \frac{\sum_i \gamma_{ik}}{\sum_i\sum_l \gamma_{il}} = \frac{\sum_i \gamma_{ik}}{N}\]</span> <span class="math display">\[\mu_k^{(t+1)} = \frac{\sum_i \gamma_{ik} x_i}{\sum_i \gamma_{ik}}\]</span> <span class="math display">\[(\sigma_k^2)^{(t+1)} = \frac{\sum_i \gamma_{ik} (x_i-\mu_k^{(t+1)})^2}{\sum_i \gamma_{ik}}\]</span> 上面的式子要推广到高维空间也很直接，只要保证相应的矩阵和矢量形状正确，不难写出正确的更新步骤。</p>
</div>
<div class="section level1">
<h1>算法实现</h1>
<p>下面我们用一维数据来实现高斯混合模型的EM算法。我们还是使用前一篇文章里的数据，真实的参数是<span class="math inline">\(\pi_1=\pi_2=0.5\)</span>, <span class="math inline">\(\mu_1=-2\)</span>, <span class="math inline">\(\mu_2=2\)</span>, <span class="math inline">\(\sigma^2_1=\sigma^2_2=1\)</span></p>
<pre class="r"><code>set.seed(1234)
x &lt;- c(rnorm(2000, -2), rnorm(2000, 2))
hist(x, breaks=500,freq = FALSE)</code></pre>
<p><img src="/post/2019-04-27-2_files/figure-html/unnamed-chunk-1-1.png" width="480" /></p>
<p>对于初始化，我们采用一个十分简单的算法</p>
<pre class="r"><code>pi1 &lt;- 0.5
pi2 &lt;- 0.5
mu1 &lt;- -0.5
mu2 &lt;- 0.5
sd1 &lt;- sd(x)
sd2 &lt;- sd(x)</code></pre>
<p>下面我们定义E-step。因为高斯分布很容易得到接近0的概率分布，为了解决这个数值稳定性问题，我们采用log-sum trick。</p>
<pre class="r"><code>mat_x &lt;- matrix(x, nrow=length(x), ncol=2)
Estep &lt;- function(param_list) {
  
  pi1 &lt;- param_list$pi1
  pi2 &lt;- param_list$pi2
  mu1 &lt;- param_list$mu1
  mu2 &lt;- param_list$mu2
  var1 &lt;- param_list$var1
  var2 &lt;- param_list$var2
  # compute aik
  mu_vec &lt;- c(mu1, mu2)
  var_vec &lt;- c(var1, var2)
  pi_vec &lt;- c(pi1, pi2)
  aik &lt;- t(mat_x) - mu_vec
  aik &lt;- -aik**2/(2*var_vec)
  aik &lt;- aik-0.5*log(var_vec)
  aik &lt;- aik + log(pi_vec)
  aik &lt;- t(aik)
  aik &lt;- aik - apply(aik, 1, max)
  gammaik &lt;- exp(aik)
  gammaik &lt;- gammaik / apply(gammaik, 1, sum)
}</code></pre>
<p>我们现在可以定义M-step</p>
<pre class="r"><code>Mstep &lt;- function(gammaik) {
  nc &lt;- colSums(gammaik)
  pis &lt;- nc / N
  pi1 &lt;- pis[1]
  pi2 &lt;- pis[2]
  
  mus &lt;- colSums(gammaik*mat_x) / nc
  mu1 &lt;- mus[1]
  mu2 &lt;- mus[2]
  
  vars &lt;- colSums((t(t(mat_x) - mus))**2 * gammaik) / nc
  var1 &lt;- vars[1]
  var2 &lt;- vars[2]
  
  return(list(pi1=pi1, pi2=pi2, mu1=mu1, mu2=mu2, var1=var1,var2=var2))
}</code></pre>
<p>有了这两个函数，我们开始估计模拟数据的参数了</p>
<pre class="r"><code>N &lt;- length(x)
param_list &lt;- list(
  pi1=pi1, pi2=pi2,
  mu1=mu1, mu2=mu2,
  var1=sd1**2, var2=sd2**2)
for (i in 1:100) {
  gammaik &lt;- Estep(param_list=param_list)
  param_list &lt;- Mstep(gammaik)
}
param_list</code></pre>
<pre><code>## $pi1
## [1] 0.499523
## 
## $pi2
## [1] 0.500477
## 
## $mu1
## [1] -2.0045
## 
## $mu2
## [1] 2.004301
## 
## $var1
## [1] 0.987977
## 
## $var2
## [1] 1.023447</code></pre>
<p>从这里我们可以看出，对于参数的估计比较准确。高斯混合模型还可以给出每个数据属于各个组分的概率<span class="math inline">\(Pr(z_{ik}=1|x,\theta)\)</span>，我们可以用这个来对数据进行聚类。 <img src="/post/2019-04-27-2_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div class="section level1">
<h1>为什么上面的算法是正确的</h1>
<p>下面我们要论证为什么上面算法是正确的。需要证明的主要内容就是在每次迭代更新中，对数似然函数是增加的，也就是 <span class="math display">\[ \ell(\theta^{(t+1)}|x) \geq \ell(\theta^{(t)}|x)\]</span> 而对数似然函数就是取得观察值的概率的对数，因此我们需要证明 <span class="math display">\[\log Pr(x|\theta^{(t+1)}) \geq \log Pr(x|\theta^{(t)})\]</span></p>
<p>我们先看不等式左边这一项。因为它是一个和隐变量<span class="math inline">\(z\)</span>无关的函数，那么它在关于<span class="math inline">\(z\)</span>的任意概率分布的均值都是它本身。取这个概率分布为<span class="math inline">\(Pr(z|x, \theta^{(t)})\)</span>我们有 <span class="math display">\[\log  Pr(x|\theta^{(t+1)}) = \underbrace{\left(\sum_z Pr(z|x, \theta^{(t)})\right)}_{=1} \log  Pr(x|\theta^{(t+1)})\log  Pr(x|\theta^{(t+1)})\]</span> 而利用贝叶斯定理，我们有 <span class="math display">\[\log  Pr(x|\theta^{(t+1)}) = \log Pr(x, z|\theta^{(t+1)}) - \log Pr(z|x, \theta^{(t+1)})\]</span> 带入上式得 <span class="math display">\[\log  Pr(x|\theta^{(t+1)}) = \sum_z Pr(z|x, \theta^{(t)}) \log Pr(x, z|\theta^{(t+1)}) -\sum_z Pr(z|x, \theta^{(t)})\log Pr(z|x, \theta^{(t+1)})\]</span></p>
<p>对于不等式右边，我们同样有 <span class="math display">\[\log  Pr(x|\theta^{(t)}) = \sum_z Pr(z|x, \theta^{(t)}) \log Pr(x, z|\theta^{(t)}) -\sum_z Pr(z|x, \theta^{(t)})\log Pr(z|x, \theta^{(t)})\]</span></p>
<p>两式相减得到 <span class="math display">\[\begin{split}
&amp; \log Pr(x|\theta^{(t+1)}) - \log Pr(x|\theta^{(t)}) \\
=&amp; \sum_z Pr(z|x, \theta^{(t)}) \log Pr(x, z|\theta^{(t+1)}) - \sum_z Pr(z|x, \theta^{(t)}) Pr(x, z|\theta^{(t)}) \\
 &amp; - \sum_z Pr(z|x, \theta^{(t)}) \log \frac{Pr(z|x, \theta^{(t+1)})}{Pr(z|x, \theta^{(t)})}
\end{split}\]</span> 等式最后一行是KL-divergence，始终为正 <span class="math display">\[- \sum_z Pr(z|x, \theta^{(t)}) \log \frac{Pr(z|x, \theta^{(t+1)})}{Pr(z|x, \theta^{(t)})} = KL(Pr(z|x, \theta^{(t)}) || Pr(z|x, \theta^{(t+1)})) \geq 0\]</span> 等式右边第一项可以写成 <span class="math display">\[\begin{split}
&amp; \sum_z Pr(z|x, \theta^{(t)}) \log Pr(x, z|\theta^{(t+1)}) \\
= &amp; \sum_z Pr(z|x, \theta^{(t)}) \sum_i \sum_k z_{ik} \left(\log \pi_k + \log\left[\phi(x_i|\mu_k, \sigma_k^2) \right]\right) \\
= &amp; \sum_i \sum_k \left(\sum_z Pr(z|x, \theta^{(t)}) z_{ik} \right) \left(\log \pi_k + \log\left[\phi(x_i|\mu_k, \sigma_k^2) \right]\right) \\
= &amp; \sum_i \sum_k \gamma_{ik} \left(\log \pi_k + \log\left[\phi(x_i|\mu_k, \sigma_k^2) \right]\right) \\
= &amp; \ell_{complete}(\theta^{(t+1)}|x, E(z|\theta^{(t)}))
\end{split}\]</span> 对于第二项也可以进行相同处理，因此我们有 <span class="math display">\[\begin{split}
&amp; \log Pr(x|\theta^{(t+1)}) - \log Pr(x|\theta^{(t)}) \\
= &amp; \ell_{complete}(\theta^{(t+1)}|x, E(z|\theta^{(t)})) - \ell_{complete}(\theta^{(t)}|x, E(z|\theta^{(t)})) \\
  &amp; + KL(Pr(z|x, \theta^{(t)}) || Pr(z|x, \theta^{(t+1)})) \\
\geq &amp; \ell_{complete}(\theta^{(t+1)}|x, E(z|\theta^{(t)})) - \ell_{complete}(\theta^{(t)}|x, E(z|\theta^{(t)}))
\end{split}\]</span> 但是在M-step中，我们更新参数使用到 <span class="math display">\[\theta^{(t+1)} = \arg\max_\theta \ell_{complete}(\theta|x, z=E(z|x, \theta^{(t)}))\]</span> 因此 <span class="math display">\[\ell_{complete}(\theta^{(t+1)}|x, E(z|\theta^{(t)})) \geq \ell_{complete}(\theta^{(t)}|x, E(z|\theta^{(t)}))\]</span> 所以我们最终证明了 <span class="math display">\[\ell(\theta^{(t+1)}|x) \geq \ell(\theta^{(t)}|x)\]</span></p>
</div>
<div class="section level1">
<h1>小结</h1>
<p>在本篇文章里，我们利用一维数据介绍了高斯混合模型的一种常用求解方法：EM算法，用代码展示了具体的计算步骤，也证明了为什么EM算法可行。</p>
<p>由于本文是介绍算法目的，代码更注重的是可读性，对于质量并没有过多追求。因此有很多可以改进的地方，比如说在<code>Estep</code>和<code>Mstep</code>两个函数中都使用了全局变量，也没有收敛性的检查。在实际生产代码中，这些都是需要改进的地方。</p>
</div>
